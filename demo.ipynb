{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the Text-to-Speech module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glados.tts as tts\n",
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TTS engine\n",
    "glados_tts = tts.TTSEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the audio.\n",
    "# Glados is spelt incorrectly on purpose to make the pronunciation more accurate.\n",
    "audio = glados_tts.generate_speech_audio(\"Hello, my name is Gladohs. I am an AI created by Aperture Science.\")\n",
    "\n",
    "# Play the audio\n",
    "sd.play(audio, tts.RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the Automatic speech recognition system\n",
    "This will detect and transcribe your voice. In this demo, it will then get GlaDOS to repeat back to you what was heard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glados.voice_recognition as vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_text(text: str):\n",
    "    \"\"\"Say text using text-to-speech engine\n",
    "    \"\"\"\n",
    "    audio = glados_tts.generate_speech_audio(text)\n",
    "    sd.play(audio, tts.RATE)\n",
    "    sd.wait()\n",
    "\n",
    "# Instantiate VoiceRecognition class with the say_text function\n",
    "demo = vr.VoiceRecognition(function=say_text)\n",
    "\n",
    "# Start the demo\n",
    "demo.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the LLM\n",
    "This allows you to interact directly with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glados.llama import LlamaServer, LlamaServerConfig\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# We have two different ways of creating a llama.cpp object.\n",
    "# Either the Llama server can be started directly from within python,\n",
    "# or we can use a reference to an external server.\n",
    "# We will start the server for this case:\n",
    "llama_server_config = LlamaServerConfig.from_yaml(Path(os.path.expanduser(\"glados_config.yml\")).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_server_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = LlamaServer.from_config(llama_server_config)\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import yaml\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "\n",
    "LLM_STOP_SEQUENCE = \"<|eot_id|>\"  # End of sentence token for Meta-Llama-3\n",
    "TEMPLATES = {\n",
    "    \"LLAMA3\": \"\".join([\n",
    "        \"{% set loop_messages = messages %}\",\n",
    "        \"{% for message in loop_messages %}\",\n",
    "        \"    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\",\n",
    "        \"    {% if loop.index0 == 0 %}\",\n",
    "        \"        {% set content = bos_token + content %}\",\n",
    "        \"    {% endif %}\",\n",
    "        \"    {{ content }}\",\n",
    "        \"{% endfor %}\",\n",
    "        \"{% if add_generation_prompt %}\",\n",
    "        \"    {{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\",\n",
    "        \"{% endif %}\"\n",
    "    ]),\n",
    "    \"CHATML\": \"\".join([\n",
    "        \"{% if messages[0]['role'] == 'system' %}\",\n",
    "        \"    {% set offset = 1 %}\",\n",
    "        \"{% else %}\",\n",
    "        \"    {% set offset = 0 %}\",\n",
    "        \"{% endif %}\",\n",
    "        \"\",\n",
    "        \"{{ bos_token }}\",\n",
    "        \"{% for message in messages %}\",\n",
    "        \"    {% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}\",\n",
    "        \"        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\",\n",
    "        \"    {% endif %}\",\n",
    "        \"\",\n",
    "        \"    {{ '<|im_start|>' + message['role'] + '\\n' + message['content'] | trim + '<|im_end|>\\n' }}\",\n",
    "        \"{% endfor %}\",\n",
    "        \"\",\n",
    "        \"{% if add_generation_prompt %}\",\n",
    "        \"    {{ '<|im_start|>assistant\\n' }}\",\n",
    "        \"{% endif %}\"\n",
    "    ])\n",
    "}\n",
    "DEFAULT_PERSONALITY_PREPROMPT = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assistant. You are here to assist the user in their tasks. The current time is {t:%l}:{t:%M} {t:%p}.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GladosConfig:\n",
    "    completion_url: str\n",
    "    api_key: Optional[str]\n",
    "    wake_word: Optional[str]\n",
    "    announcement: Optional[str]\n",
    "    personality_preprompt: List[dict[str, str]]\n",
    "    interruptible: bool\n",
    "    template: str = \"LLAMA3\"\n",
    "    voice_model: str = \"glados.onnx\"\n",
    "    speaker_id: int = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_yaml(cls, path: str, key_to_config: Sequence[str] | None = (\"Glados\",)):\n",
    "        key_to_config = key_to_config or []\n",
    "\n",
    "        with open(path, \"r\") as file:\n",
    "            data = yaml.safe_load(file)\n",
    "\n",
    "        config = data\n",
    "        for nested_key in key_to_config:\n",
    "            config = config[nested_key]\n",
    "\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "class LlamaClient:\n",
    "    @property\n",
    "    def messages(self) -> Sequence[dict[str, str]]:\n",
    "        return self._messages\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        server: LlamaServer,\n",
    "        api_key: str | None = None,\n",
    "        template: str = \"LLAMA3\",\n",
    "        personality_preprompt: Sequence[dict[str, str]] = DEFAULT_PERSONALITY_PREPROMPT\n",
    "    ):\n",
    "        self.completion_url = server.completion_url\n",
    "        # LLAMA_SERVER_HEADERS\n",
    "        self.prompt_headers = {\"Authorization\": api_key or \"Bearer your_api_key_here\"}\n",
    "        self._messages = personality_preprompt\n",
    "        self.template = Template(TEMPLATES[template])\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls,\n",
    "        server: LlamaServer,\n",
    "        config: GladosConfig\n",
    "    ):\n",
    "\n",
    "        personality_preprompt = []\n",
    "        for line in config.personality_preprompt:\n",
    "            personality_preprompt.append(\n",
    "                {\"role\": list(line.keys())[0], \"content\": list(line.values())[0]}\n",
    "            )\n",
    "\n",
    "        return cls(\n",
    "            server=server,\n",
    "            api_key=config.api_key,\n",
    "            template=config.template,\n",
    "            personality_preprompt=personality_preprompt\n",
    "        )\n",
    "        \n",
    "    def process_query(self, query):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": query})\n",
    "        now = datetime.now()\n",
    "        prompt = self.template.render(\n",
    "            messages=[{\"role\": message['role'], \"content\": message['content'].format(t=now)} for message in self.messages],\n",
    "            bos_token=\"<|begin_of_text|>\",\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        print(prompt)\n",
    "        data = {\n",
    "            \"stream\": True,\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        sentences = []\n",
    "        with requests.post(\n",
    "            self.completion_url,\n",
    "            headers=self.prompt_headers,\n",
    "            json=data,\n",
    "            stream=True\n",
    "        ) as response:\n",
    "            sentence = []\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    line = self._clean_raw_bytes(line)\n",
    "                    next_token = self._process_line(line)\n",
    "                    if next_token:\n",
    "                        # print(next_token, end=\"\")\n",
    "                        print(f\"\\x1b[36m*{next_token}* \\x1b[0m\", end=\"\")\n",
    "                        sentence.append(next_token)\n",
    "                        # If there is a pause token, print the queue so far\n",
    "                        if next_token in [\n",
    "                            \".\",\n",
    "                            \"!\",\n",
    "                            \"?\",\n",
    "                            \"?!\",\n",
    "                            \"\\n\",\n",
    "                            \"\\n\\n\"\n",
    "                        ]:\n",
    "                            sentences.append(self._process_sentence(sentence))\n",
    "                            sentence = []\n",
    "                        if next_token == \"<|im_end|>\":\n",
    "                            break\n",
    "            if sentence:\n",
    "                sentences.append(self._process_sentence(sentence))\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": \"\".join(sentences)})\n",
    "\n",
    "    def _clean_raw_bytes(self, line):\n",
    "        line = line.decode(\"utf-8\")\n",
    "        line = line.removeprefix(\"data: \")\n",
    "        line = json.loads(line)\n",
    "        return line\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        if not line['stop']:\n",
    "            token = line['content']\n",
    "            return token\n",
    "\n",
    "    def _process_sentence(self, current_sentence: List[str]):\n",
    "        sentence = \"\".join(current_sentence)\n",
    "        sentence = re.sub(r\"\\<\\|im_end\\|\\>.*$\", \"\", sentence)\n",
    "        sentence = re.sub(r\"\\*.*?\\*|\\(.*?\\)|\\<\\|.*?\\|\\>\", \"\", sentence)\n",
    "        sentence = (\n",
    "            sentence.replace(\"\\n\\n\", \". \")\n",
    "            .replace(\"\\n\", \". \")\n",
    "            .replace(\"  \", \" \")\n",
    "        )\n",
    "        if sentence:\n",
    "            print()\n",
    "            print(sentence)\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_client_config = GladosConfig.from_yaml(Path(os.path.expanduser(\"glados_config.yml\")).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_client_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaClient.from_config(server, llama_client_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.process_query(\"Hello, how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.process_query(\"What time is it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
